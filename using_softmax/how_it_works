run preprocess_text_file.py with input file path, and output file path
the output file path will contain the training data (Word_pairs)


input-pipeline is as follows:
first we build a generator function that reads a file (word_pairs) and generate x,y one hot vectors
for the whole file
then we use tf.data.Dataset.from_generator() passing that generator object, to be consumed
then we create the iterator that consumes the dataset using the from_structure method, to have more
flexibility with changing datasets (train,val,test) and still having one generator
this all well explaing in build_input_pipeline.py code
